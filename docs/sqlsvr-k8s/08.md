# 8.在 Kubernetes 上监控 SQL Server

虽然我们可以使用 kubectl 来查看特定 pod 的日志文件，但这根本不符合监控解决方案的要求，而且由于节点、pod 和容器之间工作负载的分布式和短暂性，我们真的认为有必要在问题发生时(甚至在问题发生之前)对您的环境进行集中观察。

因此，我们想向您介绍两个与 Kubernetes 和 SQL Server 完全集成的开源解决方案，Grafana 用于性能和指标监控，Kibana 用于日志聚合和分析。

虽然 Kubernetes 有一个内置的仪表板，但它不能为生产环境提供足够的功能。

Grafana 和 Kibana 都是开源工具，因此您可以查看和贡献它们的代码库。

## 用于性能监控的 Grafana

Grafana 门户提供了关于 Kubernetes 节点本身的状态和性能的性能指标和见解，以及特定于 SQL Server 的指标。

让我们先给你一个关于 Kubernetes 上的 SQL Server 的主要相关内容的概述——意思是系统指标，然后是 SQL Server 指标——然后指导你如何安装和配置这个解决方案。

### 系统度量

系统指标总结了 Kubernetes 节点的状态，包括典型的性能指标，如 CPU、RAM 和磁盘使用情况，如图 [8-1](#Fig1) 所示。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig1_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig1_HTML.jpg)

图 8-1

Grafana 门户 Telegraf 系统仪表板

除了“大图”，您还可以获得每个组件的详细信息，如特定的磁盘或网络接口。

当遇到性能问题时，这总是一个好的起点。这也可以很好地表明您是否过度配置了集群。

### SQL Server 指标

虽然主机节点指标侧重于群集的物理方面，但图 [8-2](#Fig2) 中所示的 SQL Server 指标提供了特定于 SQL Server 的性能指标，其中许多指标 DBA 已经很熟悉了。

统计数据显示等待时间、按等待类型排序的等待任务数、每秒的事务和请求数以及其他有价值的指标。它们有助于更好地了解群集中特定 SQL Server 实例的状态，可以在屏幕的左上角选择该实例。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig2_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig2_HTML.jpg)

图 8-2

Grafana 门户 SQL Server 指标

#### 如何安装和配置 Grafana

安装和配置 Grafana 不仅仅是部署一个小型 pod 那么简单。Grafana 是可视化数据的工具，因此我们还需要两个幕后组件: *InfluxDB* ，这是我们存储所有收集到的指标的数据库，以及 *Telegraf* ，它将被部署到每个节点，收集这些节点上的指标，并将其发送到 *InfluxDB* 。

设置 Grafana 所需的步骤是

*   创建一个名称空间，并将 is 设置为当前上下文。

*   设置您的变量和秘密供 InfluxDB 和 Grafana 使用。

*   为 InfluxDB 配置存储。

*   部署 InfluxDB。

*   通过服务公开 InfluxDB 部署。

*   为 Telegraf 创建配置图。

*   为 Telegraf 创建 DaemonSet。

*   为 Grafana 提供存储空间。

*   部署格拉夫纳。

*   通过服务公开 Grafana 部署。

*   通过 Grafana 门户配置 Grafana。

你可以在图 [8-3](#Fig3) 中看到这个合奏。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig3_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig3_HTML.jpg)

图 8-3

Grafana 及其所需组件

我们不会指导您完成或解释推出这些组件的每一个选项，仅这一点就足以形成一本书，而是指导您一步一步地完成所需的任务，让它们在您的集群中运行，以监控您的 SQL Server 工作负载。

为了保持整洁，让我们首先创建一个名为 *monitoring* 的名称空间，并使用清单 [8-1](#PC1) 中的命令将其设置为当前上下文。这意味着我们在此之后部署的每个对象都将自动部署到这个名称空间。

```
kubectl create namespace monitoring
kubectl config set-context --current --namespace=monitoring

Listing 8-1Create and switch to namespace “monitoring”

```

InfluxDB 将需要几个秘密，我们将使用清单 [8-2](#PC2) 中的命令来设置。

```
kubectl create secret generic influxdb-creds \
--from-literal=INFLUXDB_DB=monitoring \
--from-literal=INFLUXDB_USER=user \
--from-literal=INFLUXDB_USER_PASSWORD=user\
--from-literal=INFLUXDB_READ_USER=readonly \
--from-literal=INFLUXDB_READ_USER_PASSWORD=readonly \
--from-literal=INFLUXDB_ADMIN_USER=admin \
--from-literal=INFLUXDB_ADMIN_USER_PASSWORD=admin \
--from-literal=INFLUXDB_HOST=influxdb \
--from-literal=INFLUXDB_HTTP_AUTH_ENABLED=false

Listing 8-2Set variables and secrets for InfluxDB

```

这同样适用于 Grafana(列表 [8-3](#PC3) )。

```
kubectl create secret generic grafana-creds \
--from-literal=GF_SECURITY_ADMIN_USER=admin \
--from-literal=GF_SECURITY_ADMIN_PASSWORD=admin123 \
--from-literal=GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-clock-panel

Listing 8-3Set variables and secrets for Grafana

```

接下来是一组 YAML 旅客名单。除非另有说明，否则请将它们分别保存到一个文件中，并使用 *kubectl apply -f* 将它们部署到您的集群中。这些文件当然也可以在本书的下载中找到。

与我们对 SQL Server 实例所做的类似，让我们使用清单 [8-4](#PC4) 中的清单为 InfluxDB 创建一个持久卷和一个持久卷声明。我们将再次在 NFS 共享上存储这些数据。

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs-influxdb
  labels:
    disk: influxdb
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: storage
    path: "/srv/exports/volumes/influxdb"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nfs-influxdb
spec:
  selector:
    matchLabels:
      disk: influxdb
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

Listing 8-4influxdb-storage.yaml

```

清单 [8-5](#PC5) 部署 InfluxDB 应用程序和数据库。

```
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: monitoring
  labels:
    app: influxdb
  name: influxdb
spec:

  replicas: 1
  selector:
    matchLabels:
      app: influxdb
  template:
    metadata:
      labels:
        app: influxdb
    spec:
      containers:
      - envFrom:
        - secretRef:
            name: influxdb-creds
        image: docker.io/influxdb:1.8
        name: influxdb
        volumeMounts:
        - mountPath: /var/lib/influxdb
          name: var-lib-influxdb
      volumes:
      - name: var-lib-influxdb
        persistentVolumeClaim:
          claimName: pvc-nfs-influxdb

Listing 8-5influxdb-deployment.yaml

```

我们将使用清单 [8-6](#PC6) 中所示的节点端口服务来公开这个部署。

Note

在这个清单中，我们为我们的服务定义了一个静态端口(30010)。与以前部署的服务不同，此服务将始终在此端口上运行，而不是在随机端口上运行。这样做的好处是你不必改变你的 URL 调用。

```
apiVersion: v1
kind: Service
metadata:
  labels:
    app: influxdb
  name: influxdb
  namespace: monitoring
spec:
  ports:
  - port: 8086
    protocol: TCP
    targetPort: 8086
    nodePort: 30010
  selector:
    app: influxdb
  type: NodePort 

Listing 8-6influxdb-service.yaml

```

清单 [8-7](#PC7) 提供了 Telegraf 的配置图。我们添加了最常见的系统指标(如 CPU、磁盘和内存)以及我们的 SQL Server。

Note

在应用之前，请调整此清单中的<port>和<password>以匹配您的设置。这将是您用于 SQL Server 及其端口(节点端口)的密码。</password></port>

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: telegraf
  namespace: monitoring
  labels:
    k8s-app: telegraf
data:
  telegraf.conf: |+
    [global_tags]
      env = "kubeadm"
    [agent]
      hostname = "$HOSTNAME"
    [[outputs.influxdb]]
      urls = ["http://$INFLUXDB_HOST:8086/"] # required
      database = "$INFLUXDB_DB" # required
      timeout = "5s"
      username = "$INFLUXDB_USER"
      password = "$INFLUXDB_USER_PASSWORD"
    [[inputs.cpu]]
      percpu = true
      totalcpu = true
      collect_cpu_time = false
      report_active = false
    [[inputs.disk]]
      ignore_fs = ["tmpfs", "devtmpfs", "devfs"]
    [[inputs.diskio]]
    [[inputs.kernel]]
    [[inputs.mem]]
    [[inputs.processes]]
    [[inputs.swap]]
    [[inputs.system]]
    [[inputs.docker]]
      endpoint = "unix:///var/run/docker.sock"
    [[inputs.sqlserver]]
      servers = [

      "Server=control;Port=<Port>;User Id=sa;Password=<Password>;app name=telegraf;log=1;"
      ]
      query_version = 2

Listing 8-7telegraf-configmap.yaml

```

接下来，我们将使用清单 [8-8](#PC8) 通过 DaemonSet 部署 Telegraf。

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: telegraf
  namespace: monitoring
  labels:
    k8s-app: telegraf
spec:
  selector:
    matchLabels:
      name: telegraf
  template:

    metadata:
      labels:
        name: telegraf
    spec:
      containers:
      - name: telegraf
        image: docker.io/telegraf:latest
        env:
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: "HOST_PROC"
          value: "/rootfs/proc"
        - name: "HOST_SYS"
          value: "/rootfs/sys"
        - name: INFLUXDB_USER
          valueFrom:
            secretKeyRef:
              name: influxdb-creds
              key: INFLUXDB_USER
        - name: INFLUXDB_USER_PASSWORD
          valueFrom:
            secretKeyRef:
              name: influxdb-creds
              key: INFLUXDB_USER_PASSWORD
        - name: INFLUXDB_HOST
          valueFrom:
            secretKeyRef:
              name: influxdb-creds
              key: INFLUXDB_HOST
        - name: INFLUXDB_DB
          valueFrom:
            secretKeyRef:
              name: influxdb-creds
              key: INFLUXDB_DB
        volumeMounts:
        - name: sys
          mountPath: /rootfs/sys
          readOnly: true
        - name: proc
          mountPath: /rootfs/proc
          readOnly: true
        - name: docker-socket
          mountPath: /var/run/docker.sock
        - name: utmp
          mountPath: /var/run/utmp
          readOnly: true
        - name: config
          mountPath: /etc/telegraf
      terminationGracePeriodSeconds: 30
      volumes:
      - name: sys
        hostPath:
          path: /sys
      - name: docker-socket
        hostPath:
          path: /var/run/docker.sock
      - name: proc

        hostPath:
          path: /proc
      - name: utmp
        hostPath:
          path: /var/run/utmp
      - name: config
        configMap:
          name: telegraf

Listing 8-8telegraf-daemonset.yaml

```

既然 InfluxDB 存储了我们的指标，Telegraf 也为我们收集了这些指标，那么是时候推出 Grafana 来可视化它们了。

清单 [8-9](#PC9) 中的代码将首先为我们提供 Grafana 配置的存储。

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-nfs-grafana
  labels:
    disk: grafana
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  nfs:
    server: storage
    path: "/srv/exports/volumes/grafana"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-nfs-grafana
spec:
  selector:
    matchLabels:
      disk: grafana
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

Listing 8-9grafana-storage.yaml

```

接下来使用清单 [8-10](#PC10) 部署 Grafana 本身。请注意，Grafana 需要自己的用户和组来运行。我们已经在第 1 章[建立了 NFS 服务器。](01.html)

```
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: monitoring
  labels:
    app: grafana
  name: grafana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:

    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - envFrom:
        - secretRef:
            name: grafana-creds
        image: docker.io/grafana/grafana:7.3.3
        name: grafana
        volumeMounts:
          - name: data-dir
            mountPath: /var/lib/grafana/
      securityContext:
        fsGroup: 472
        runAsGroup: 472
        runAsNonRoot: true
        runAsUser: 472

      volumes:
       - name: data-dir
         persistentVolumeClaim:
           claimName: pvc-nfs-grafana

Listing 8-10grafana-deployment.yaml

```

在我们的最后一步，我们将使用清单 [8-11](#PC11) 将 Grafana 作为服务公开。该服务将再次使用静态端口(30080)。

```
apiVersion: v1
kind: Service
metadata:
  labels:
    app: grafana
  name: grafana
  namespace: monitoring
spec:
  ports:
  - port: 443
    name: https
    targetPort: 3000
    nodePort: 30080
  selector:
    app: grafana
  type: NodePort

Listing 8-11grafana-service.yaml

```

您的性能监控环境现在可以使用了。打开 web 浏览器并登录到 [`http://control:30080`](http://www.control:30080) (这可能发生在能够到达控制平面节点并在其主机文件中具有匹配条目的任何机器上)。使用我们之前定义的凭证(在我们的示例 admin 和 admin123 中)。

导航至左侧*配置* ➤ *数据源*(图 [8-4](#Fig4) )点击*添加数据源*。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig4_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig4_HTML.jpg)

图 8-4

graf ana–添加数据源

从*时间序列数据库*中勾选 *InfluxDB* ，点击*选择*，如图 [8-5](#Fig5) 所示。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig5_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig5_HTML.jpg)

图 8-5

grafana–添加数据源-时间序列数据库

将网址(图 [8-6](#Fig6) )设置为 [`http://influxdb:8086`](http://www.influxdb:8086) 。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig6_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig6_HTML.jpg)

图 8-6

graf ana–添加数据源-设置

向下滚动，将*数据库*设置为*监控、*，点击*保存&测试*，验证您的连接，如图 [8-7](#Fig7) 所示。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig7_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig7_HTML.jpg)

图 8-7

graf ana–添加数据源

使用这个连接，我们现在可以使用它的指标创建一个仪表板。同样，我们不会深入讨论这个问题，而是使用两个现有的控制面板。

导航到*仪表盘*，选择*管理*，如图 [8-8](#Fig8) 所示。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig8_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig8_HTML.jpg)

图 8-8

grafana–管理仪表板

在下一个屏幕上(图 [8-9](#Fig9) ，点击*导入*。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig9_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig9_HTML.jpg)

图 8-9

grafana–导入仪表板

在上段*通过*grafana.com 导入，输入 ID *928* ，点击*加载*(图 [8-10](#Fig10) )。仪表板 928 是来自 Grafana 库的预配置系统仪表板，我们将在本章稍后向您解释如何找到其他仪表板供您使用。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig10_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig10_HTML.jpg)

图 8-10

grafana 导入仪表板(dashboad id)

将屏幕下方的 *InfluxDB telegraf* 下拉菜单(图 [8-11](#Fig11) )更改为 *InfluxDB* 并点击*导入*。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig11_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig11_HTML.jpg)

图 8-11

grafana–导入仪表板(结果)

这将为您导入并打开 Telegraf 系统仪表板，其中包含大量关于您的 Kubernetes 集群背后的硬件的宝贵信息(参见图 [8-12](#Fig12) )。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig12_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig12_HTML.jpg)

图 8-12

grafana–电报控制板

使用仪表板 ID *9386* 重复相同的步骤，以导入具有 SQL Server 特定指标的仪表板。

在 [`https://grafana.com/ grafana/dashboards/`](https://grafana.com/%2520grafana/dashboards/) ，你会发现更多可以使用的仪表盘。如果你想了解更多关于 Grafana 本身的信息，请查看 [`https://grafana.com/`](https://grafana.com/) 。

## 用于日志聚合和管理的 Kibana

另一方面，图 [8-13](#Fig13) 所示的 Kibana 仪表板让您能够深入了解 Kubernetes 日志文件，包括那些影响 SQL Server pods 和容器的文件。日志在故障排除场景中很有价值，考虑到日志与 pod 的生命周期相关联，我们需要独立访问日志的方法，我们需要一个解决方案来解决这个问题。基巴纳是弹性堆栈的一部分。它还提供了在日志文件上创建可视化和仪表板的选项。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig13_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig13_HTML.jpg)

图 8-13

基巴纳门户-概述

### 如何安装和配置 Kibana

Kibana 使用与 Grafana 完全不同的组件，但总体逻辑非常相似。正如你在图 [8-14](#Fig14) 中看到的，我们有 *elasticsearch* 作为我们的数据存储，而 Kibana 被用作前端。*每个节点上的 Fluent-bit* Pods 负责收集日志并发送给 *elasticsearch* 。

设置 Kibana 所需的步骤是

*   创建一个名称空间，并将其设置为当前上下文。

*   部署 elasticsearch 并将其作为服务公开。

*   部署流畅位。

*   部署 Kibana 并将其作为服务公开。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig14_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig14_HTML.jpg)

图 8-14

Kibana 及其所需组件

就像之前的 Grafana 一样，让我们使用清单 [8-12](#PC12) 中的代码为 Kibana 创建一个新的名称空间。

```
kubectl create namespace logging
kubectl config set-context --current --namespace=logging

Listing 8-12Create and switch to namespace “logging”

```

同样，就像 Grafana 一样，我们不会解释每一个细节，但会引导您完成将日志聚合解决方案部署到集群的步骤。

我们将从在我们的集群中公开 elasticsearch 的服务开始(清单 [8-13](#PC13) )。

你注意到什么了吗？我们在部署豆荚之前公开服务。这只是为了说明这里的顺序无关紧要。当然，只有在创建了目标部署之后，才可以访问该服务。

```
kind: Service
apiVersion: v1
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
spec:
  selector:
    app: elasticsearch
  clusterIP: None
  ports:
    - port: 9200
      name: rest
    - port: 9300
      name: inter-node

Listing 8-13elasticsearch-service.yaml

```

由于 elasticsearch 使用 StatefulSets，它要求每个 pod 有一个持久卷，因此我们将使用一个*存储类*进行动态预配置，这将再次指向我们的 NFS 服务器，而不是静态预配置一个 *PV* 和 *PVC* 。第 [6](06.html) 章中已经提供了该存储类别。

存储就绪后，我们可以使用清单 [8-14](#PC14) 中的清单为我们的 elasticsearch 应用程序推出一个 StatefulSet。在这个清单的最后一部分，您可以看到我们的 *nfs-storage* 存储类被引用，这就是两者是如何链接在一起的。

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: es-cluster
  namespace: logging
spec:
  serviceName: elasticsearch

replicas: 3
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0
        resources:
            limits:
              cpu: 1000m
            requests:
              cpu: 100m
        ports:
        - containerPort: 9200
          name: rest
          protocol: TCP
        - containerPort: 9300
          name: inter-node
          protocol: TCP
        volumeMounts:
        - name: data
          mountPath: /usr/share/elasticsearch/data
        env:
          - name: cluster.name
            value: k8s-logs
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: discovery.seed_hosts
            value: "es-cluster-0.elasticsearch,es-cluster-1.elasticsearch,es-cluster-2.elasticsearch"
          - name: cluster.initial_master_nodes
            value: "es-cluster-0,es-cluster-1,es-cluster-2"
          - name: ES_JAVA_OPTS
            value: "-Xms512m -Xmx512m"
      initContainers:
      - name: fix-permissions
        image: busybox
        command: ["sh", "-c", "chown -R 1000:1000 /usr/share/elasticsearch/data"]
        securityContext:
          privileged: true
        volumeMounts:
        - name: data

          mountPath: /usr/share/elasticsearch/data
      - name: increase-vm-max-map
        image: busybox
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        securityContext:
          privileged: true
      - name: increase-fd-ulimit
        image: busybox
        command: ["sh", "-c", "ulimit -n 65536"]
        securityContext:
          privileged: true
  volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        app: elasticsearch
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: nfs-storage
      resources:
        requests:
          storage: 100Gi

Listing 8-14Deploy elasticsearch as a StatefulSet (elasticsearch.yaml)

```

随着 elasticsearch 在我们的集群上运行，我们可以使用清单 [8-15](#PC15) 中的清单部署 Kibana 并将其作为服务公开(同样使用静态端口:30445)。这也是一个例子，表明我们是通过多个清单还是单个清单来部署所有东西(这里是服务和部署)并不重要。此外，我们在创建部署之前就创建了服务，但是 Kubernetes 为我们无缝地完成了这项工作。

```
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
spec:
  ports:
  - port: 5601
    targetPort: 5601
    nodePort: 30445
  selector:
    app: kibana
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      containers:
      - name: kibana
        image: docker.elastic.co/kibana/kibana:7.2.0
        resources:
          limits:
            cpu: 1000m
          requests:
            cpu: 100m
        env:

          - name: ELASTICSEARCH_URL
            value: http://elasticsearch:9200
        ports:
        - containerPort: 5601

Listing 8-15kibana.yaml

```

现在我们用 elasticsearch 保存数据，用 Kibana 显示数据。唯一缺少的组件是收集日志的 fluent-bit，所以我们有东西可以使用。

在这种情况下，我们通过直接从 GitHub 应用一个文件来创建一切，从 ServiceAccount 到 Roles 和部署(清单 [8-16](#PC16) )。您也可以先保存它们，然后应用本地清单，以防您想先检查它们或进行任何更改。

```
kubectl create -f https://raw.githubusercontent.com/fluent/fluent-bit-kubernetes-logging/master/fluent-bit-service-account.yaml
kubectl create -f https://raw.githubusercontent.com/fluent/fluent-bit-kubernetes-logging/master/fluent-bit-role.yaml
kubectl create -f https://raw.githubusercontent.com/fluent/fluent-bit-kubernetes-logging/master/fluent-bit-role-binding.yaml
kubectl create -f https://raw.githubusercontent.com/fluent/fluent-bit-kubernetes-logging/master/output/elasticsearch/fluent-bit-configmap.yaml
kubectl create -f https://raw.githubusercontent.com/fluent/fluent-bit-kubernetes-logging/master/output/elasticsearch/fluent-bit-ds.yaml

Listing 8-16Install fluent-bit

```

我们的日志聚合系统现在已经可以使用了。打开网络浏览器，导航至 [`http://control:30445/`](http://www.control:30445/) 。

系统会询问您(参见图 [8-15](#Fig15) )是想从样本数据开始还是自己探索，您将点击*自己探索*。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig15_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig15_HTML.jpg)

图 8-15

基贝拉入口

与 Grafana 类似，我们需要为 Kibana 配置数据源，然后才能使用它们。如图 [8-16](#Fig16) 所示，点击左侧菜单中的*发现*。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig16_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig16_HTML.jpg)

图 8-16

基巴纳门户导航

配置菜单(图 [8-17](#Fig17) )将要求您定义一个步进模式。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig17_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig17_HTML.jpg)

图 8-17

Kibana 门户-定义索引模式

我们现在保持简单，所以在文本框中输入 *logstash-** 并点击*下一步*。

下一步(图 [8-18](#Fig18) )要求您配置哪个字段将用于按时间过滤日志数据。在下拉列表中选择*@时间戳*，点击*创建索引模式*。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig18_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig18_HTML.jpg)

图 8-18

Kibana 门户-配置索引设置

再次点击左侧的 *Discover* ，您将会看到您的集群内收集的日志的直方图(图 [8-19](#Fig19) )。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig19_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig19_HTML.jpg)

图 8-19

基巴纳直方图

这些数据可以通过多种不同的方式进行过滤和分析。想要了解更多关于 Kibana 的信息，我们推荐你去看一看 [`www.elastic.co/kibana`](https://www.elastic.co/kibana) 。

### Kibana 中的 SQL Server 日志

我们想要明确指出的一组过滤器是如何在 Kibana 中检索 SQL Server 实例的日志。

一个非常简单的方法就是使用自由文本搜索，如图 8-20 所示。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig20_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig20_HTML.jpg)

图 8-20

Kibana 中的自由文本过滤器

这将涉及许多不必要的噪音。更好的解决方案是添加一个合适的过滤器。为此，点击过滤栏下方的*添加过滤器*，如图 [8-21](#Fig21) 所示。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig21_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig21_HTML.jpg)

图 8-21

在 Kibana 中添加过滤器

如图 [8-22](#Fig22) 选择字段*kuberneters . container _ name*，运算符 *is、*和值 *mssql* ，点击*保存*。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig22_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig22_HTML.jpg)

图 8-22

在 Kibana 中为 mssql 添加过滤器

这将从我们所有的 SQL Server 容器中调出日志文件，如图 [8-23](#Fig23) 所示。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig23_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig23_HTML.jpg)

图 8-23

Kibana 中 mssql 的过滤器

当然，这也可以与自由文本过滤器结合使用。如果您将*“登录失败”*添加到如图 [8-24](#Fig24) 所示的过滤器中，您将在我们的 SQL Server 中获得所有失败的登录事件，因为该日志还包括完整的 SQL Server 错误日志。

![../images/494804_1_En_8_Chapter/494804_1_En_8_Fig24_HTML.jpg](../images/494804_1_En_8_Chapter/494804_1_En_8_Fig24_HTML.jpg)

图 8-24

Kibana 中的 SQL Server 错误日志

在我们的例子中，这已经足够了。如果您正在运行多个 SQL Server 实例，您还可以为设置添加额外的过滤器，如*名称空间*、*部署、*或 *pod* 名称。

## 清除

为了确保您接下来的演示在正确的名称空间中运行，将当前上下文重置为默认名称空间，如清单 [8-17](#PC17) 所示。

```
kubectl config set-context --current --namespace=default

Listing 8-17Reset kubectl context

```

Note

本例中使用的端口也将被后续章节中的服务使用。请记住这一点并修改这些章节中的端口，或者首先删除本章中的资源。

## 摘要

在本章中，我们介绍了如何使用 Grafana 和 Kibana 仪表板来监控集群及其应用程序。然后，我们使用这些门户来深入了解我们之前在 Kubernetes 上部署的 SQL Server。让我们进入下一章，看看支持 Azure Arc 的数据服务，它允许我们在 Kubernetes 上部署具有高可用性选项的 SQL Server。